#!/usr/bin/env python3
"""
Experiment: Can activation probes determine if a question is math vs non-math?

This script:
1. Loads MMLU questions from math and non-math subjects
2. Generates completions and extracts activations
3. Trains probes to classify questions as math vs non-math
4. Evaluates probe performance with AUROC and visualizations

Math subjects: abstract_algebra, college_mathematics, elementary_mathematics, 
               high_school_mathematics, high_school_statistics
Non-math subjects: All other MMLU subjects
"""

import os
import json
import argparse
import multiprocessing as mp
import numpy as np

import config
from lib.data import load_mmlu_data
from lib.generation import generate_with_vllm_multi_server
from lib.activations import extract_all_layers_all_positions_multi_gpu
from lib.probes import train_linear_probe
from lib.visualization import (
    plot_roc_curve,
    plot_score_distribution,
    plot_pca
)


# Define math vs non-math subjects
MATH_SUBJECTS = {
    'abstract_algebra',
    'college_mathematics',
    'elementary_mathematics',
    'high_school_mathematics',
    'high_school_statistics'
}

NON_MATH_SUBJECTS = {
    'anatomy',
    'astronomy',
    'business_ethics',
    'clinical_knowledge',
    'college_biology',
    'college_chemistry',
    'college_computer_science',
    'college_medicine',
    'college_physics',
    'computer_security',
    'conceptual_physics',
    'econometrics',
    'electrical_engineering',
    'global_facts',
    'high_school_biology',
    'high_school_chemistry',
    'high_school_computer_science',
    'high_school_european_history',
    'high_school_geography',
    'high_school_government_and_politics',
    'high_school_macroeconomics',
    'high_school_microeconomics',
    'high_school_physics',
    'high_school_psychology',
    'high_school_us_history',
    'high_school_world_history',
    'human_aging',
    'human_sexuality',
    'international_law',
    'jurisprudence',
    'logical_fallacies',
    'machine_learning',
    'management',
    'marketing',
    'medical_genetics',
    'miscellaneous',
    'moral_disputes',
    'moral_scenarios',
    'nutrition',
    'philosophy',
    'prehistory',
    'professional_accounting',
    'professional_law',
    'professional_medicine',
    'professional_psychology',
    'public_relations',
    'security_studies',
    'sociology',
    'us_foreign_policy',
    'virology',
    'world_religions'
}


def load_balanced_math_nonmath_data(
    mmlu_file: str,
    num_examples_per_class: int
):
    """
    Load balanced dataset of math and non-math questions.
    
    Args:
        mmlu_file: Path to MMLU data file
        num_examples_per_class: Number of examples to load per class
        
    Returns:
        questions: List of question dictionaries
        prompts: List of formatted prompts
        labels: Binary labels (1=math, 0=non-math)
    """
    print(f"\nLoading balanced math/non-math dataset...")
    print(f"  Target: {num_examples_per_class} examples per class")
    
    math_questions = []
    math_prompts = []
    nonmath_questions = []
    nonmath_prompts = []
    
    # Load all questions and separate by category
    all_questions, all_prompts = load_mmlu_data(mmlu_file, num_examples=None, prompt_name="benign")
    
    for q, p in zip(all_questions, all_prompts):
        subject = q['subject']
        
        if subject in MATH_SUBJECTS and len(math_questions) < num_examples_per_class:
            math_questions.append(q)
            math_prompts.append(p)
        elif subject in NON_MATH_SUBJECTS and len(nonmath_questions) < num_examples_per_class:
            nonmath_questions.append(q)
            nonmath_prompts.append(p)
        
        # Stop once we have enough of both
        if len(math_questions) >= num_examples_per_class and len(nonmath_questions) >= num_examples_per_class:
            break
    
    # Combine and create labels
    questions = math_questions + nonmath_questions
    prompts = math_prompts + nonmath_prompts
    labels = np.array([1] * len(math_questions) + [0] * len(nonmath_questions))
    
    print(f"  Loaded {len(math_questions)} math questions")
    print(f"  Loaded {len(nonmath_questions)} non-math questions")
    print(f"  Total: {len(questions)} questions")
    
    # Verify subject distribution
    math_subjects_found = set(q['subject'] for q in math_questions)
    nonmath_subjects_found = set(q['subject'] for q in nonmath_questions)
    print(f"  Math subjects: {sorted(math_subjects_found)}")
    print(f"  Non-math subjects (sample): {sorted(list(nonmath_subjects_found))[:10]}")
    
    return questions, prompts, labels


def run_math_vs_nonmath_experiment(
    model_name: str = None,
    layer_idx: int = None,
    token_position: str = None,
    num_examples_per_class: int = 100,
    max_new_tokens: int = None,
    temperature: float = None,
    num_gpus: int = None,
    mmlu_file: str = "mmlu_data/test.json"
):
    """
    Run complete math vs non-math classification experiment.
    
    Args:
        model_name: Model name
        layer_idx: Layer index to extract activations from
        token_position: Token position to extract
        num_examples_per_class: Number of examples per class
        max_new_tokens: Max tokens to generate
        temperature: Sampling temperature
        num_gpus: Number of GPUs
        mmlu_file: Path to MMLU data file
    """
    # Use config defaults
    model_name = model_name or config.MODEL_NAME
    layer_idx = layer_idx if layer_idx is not None else config.DEFAULT_LAYER
    token_position = token_position or config.DEFAULT_TOKEN_POSITION
    max_new_tokens = max_new_tokens or config.MAX_NEW_TOKENS
    temperature = temperature if temperature is not None else config.TEMPERATURE
    num_gpus = num_gpus or config.VLLM_NUM_SERVERS
    
    # Create output directories
    experiment_dir = f"{config.BASE_DIR}/{config.MODEL_SHORT_NAME}/math_vs_nonmath"
    os.makedirs(experiment_dir, exist_ok=True)
    
    print("\n" + "=" * 80)
    print("MATH vs NON-MATH CLASSIFICATION EXPERIMENT")
    print("=" * 80)
    config.print_config()
    print(f"\nExperiment Parameters:")
    print(f"  Layer: {layer_idx}")
    print(f"  Token Position: {token_position}")
    print(f"  Examples per class: {num_examples_per_class}")
    print(f"  Max New Tokens: {max_new_tokens}")
    print(f"  Temperature: {temperature}")
    print(f"  Output: {experiment_dir}")
    print("=" * 80)
    
    # Step 1: Load balanced dataset
    print(f"\n{'=' * 80}")
    print(f"STEP 1: Loading Balanced Dataset")
    print(f"{'=' * 80}")
    questions, prompts, labels = load_balanced_math_nonmath_data(
        mmlu_file, num_examples_per_class
    )
    
    # Step 2: Generate completions
    print(f"\n{'=' * 80}")
    print(f"STEP 2: Generating Completions")
    print(f"{'=' * 80}")
    generated_texts, completions_only = generate_with_vllm_multi_server(
        prompts=prompts,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        model_name=model_name,
        num_servers=num_gpus,
        base_port=config.VLLM_BASE_PORT,
        max_concurrent_requests=config.MAX_CONCURRENT_REQUESTS_PER_SERVER,
        use_cache=True,
        model_short_name=config.MODEL_SHORT_NAME,
        prompt_name="benign",
        subject_type="math_and_nonmath"  # This experiment uses both math and non-math subjects
    )
    
    # Step 3: Extract activations
    print(f"\n{'=' * 80}")
    print(f"STEP 3: Extracting Activations")
    print(f"{'=' * 80}")
    layer_position_activations, probed_tokens_by_position = extract_all_layers_all_positions_multi_gpu(
        model_name=model_name,
        full_texts=generated_texts,
        layer_indices=[layer_idx],  # Single layer in a list
        positions_to_extract=[token_position],  # Single position in a list
        num_gpus=num_gpus,
        batch_size=config.ACTIVATION_BATCH_SIZE,
        use_model_cache=True,
        gpu_ids=config.ACTIVATION_GPUS,  # Use dedicated activation GPUs
        completions=completions_only
    )
    # Extract the single layer/position result
    activations = layer_position_activations[layer_idx][token_position]
    probed_tokens = probed_tokens_by_position[token_position]
    
    print(f"\n{'=' * 80}")
    print(f"Activations shape: {activations.shape}")
    print(f"  Samples: {activations.shape[0]}")
    print(f"  Features: {activations.shape[1]}")
    print(f"  Labels: {np.sum(labels)} math, {len(labels) - np.sum(labels)} non-math")
    print(f"{'=' * 80}")
    
    # Step 4: Train and evaluate probes
    print(f"\n{'=' * 80}")
    print(f"STEP 4: Training Probes")
    print(f"{'=' * 80}")
    
    # Shuffle for random split
    np.random.seed(config.PROBE_RANDOM_STATE)
    idx = np.random.permutation(len(activations))
    activations = activations[idx]
    labels = labels[idx]
    questions = [questions[i] for i in idx]
    prompts = [prompts[i] for i in idx]
    generated_texts = [generated_texts[i] for i in idx]
    
    # Split train/test (50/50)
    split = len(activations) // 2
    X_train, X_test = activations[:split], activations[split:]
    y_train, y_test = labels[:split], labels[split:]
    
    print(f"  Training set: {len(X_train)} samples")
    print(f"    Math: {np.sum(y_train)}, Non-math: {len(y_train) - np.sum(y_train)}")
    print(f"  Test set: {len(X_test)} samples")
    print(f"    Math: {np.sum(y_test)}, Non-math: {len(y_test) - np.sum(y_test)}")
    
    # Train linear probe
    auroc, clf, y_pred_proba, fpr, tpr = train_linear_probe(
        X_train, y_train, X_test, y_test,
        max_iter=config.PROBE_MAX_ITER,
        random_state=config.PROBE_RANDOM_STATE
    )
    
    print(f"\n  ✓ Linear Probe AUROC: {auroc:.4f}")
    
    # Compute additional metrics
    y_pred = (y_pred_proba >= 0.5).astype(int)
    accuracy = np.mean(y_pred == y_test)
    print(f"  ✓ Accuracy (threshold=0.5): {accuracy:.4f}")
    
    # Step 5: Save results and visualizations
    print(f"\n{'=' * 80}")
    print(f"STEP 5: Saving Results")
    print(f"{'=' * 80}")
    
    # Create output filename prefix
    output_prefix = f"math_vs_nonmath_layer{layer_idx:02d}_pos-{token_position}_n{num_examples_per_class}x2"
    
    # Save activations and labels (both .npy and .json formats)
    np.save(os.path.join(experiment_dir, f"{output_prefix}_activations.npy"), activations)
    with open(os.path.join(experiment_dir, f"{output_prefix}_activations.json"), 'w') as f:
        json.dump(activations.tolist(), f)
    
    np.save(os.path.join(experiment_dir, f"{output_prefix}_labels.npy"), labels)
    with open(os.path.join(experiment_dir, f"{output_prefix}_labels.json"), 'w') as f:
        json.dump(labels.tolist(), f, indent=2)
    
    np.save(os.path.join(experiment_dir, f"{output_prefix}_prompts.npy"), np.array(prompts))
    with open(os.path.join(experiment_dir, f"{output_prefix}_prompts.json"), 'w') as f:
        json.dump(prompts, f, indent=2)
    
    np.save(os.path.join(experiment_dir, f"{output_prefix}_generated.npy"), np.array(generated_texts))
    with open(os.path.join(experiment_dir, f"{output_prefix}_generated.json"), 'w') as f:
        json.dump(generated_texts, f, indent=2)
    
    with open(os.path.join(experiment_dir, f"{output_prefix}_questions.json"), 'w') as f:
        json.dump(questions, f, indent=2)
    
    print(f"  ✓ Saved activations and data")
    
    # Save probe results
    results = {
        "model_name": model_name,
        "model_short_name": config.MODEL_SHORT_NAME,
        "layer_idx": layer_idx,
        "token_position": token_position,
        "num_examples_per_class": num_examples_per_class,
        "total_examples": len(labels),
        "auroc": float(auroc),
        "accuracy": float(accuracy),
        "train_size": len(X_train),
        "test_size": len(X_test),
        "description": "Binary classification: Math (1) vs Non-Math (0) questions"
    }
    
    results_file = os.path.join(experiment_dir, f"{output_prefix}_results.json")
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"  ✓ Saved results: {results_file}")
    
    # Generate visualizations
    print(f"\n  Generating visualizations...")
    
    # ROC curve
    plot_roc_curve(
        fpr, tpr, auroc,
        os.path.join(experiment_dir, f"{output_prefix}_roc.png"),
        f"ROC Curve - Math vs Non-Math (Layer {layer_idx})"
    )
    print(f"  ✓ ROC curve")
    
    # Score distribution
    plot_score_distribution(
        clf.decision_function(X_test), y_test,
        os.path.join(experiment_dir, f"{output_prefix}_scores.png"),
        f"Score Distribution - Math vs Non-Math (Layer {layer_idx})"
    )
    print(f"  ✓ Score distribution")
    
    # PCA visualization
    plot_pca(
        X_test, y_test,
        os.path.join(experiment_dir, f"{output_prefix}_pca.png"),
        f"PCA - Math vs Non-Math (Layer {layer_idx})"
    )
    print(f"  ✓ PCA visualization")
    
    # Save confusion examples
    print(f"\n  Analyzing prediction errors...")
    misclassified_idx = np.where(y_pred != y_test)[0]
    print(f"  Misclassified: {len(misclassified_idx)}/{len(y_test)} ({100*len(misclassified_idx)/len(y_test):.1f}%)")
    
    # Save examples of errors
    error_analysis = []
    for idx in misclassified_idx[:10]:  # Save first 10 errors
        global_idx = split + idx
        error_analysis.append({
            "predicted": "math" if y_pred[idx] == 1 else "non-math",
            "actual": "math" if y_test[idx] == 1 else "non-math",
            "confidence": float(y_pred_proba[idx]),
            "subject": questions[global_idx]['subject'],
            "question": questions[global_idx]['question'][:200] + "..."  # Truncate
        })
    
    error_file = os.path.join(experiment_dir, f"{output_prefix}_errors.json")
    with open(error_file, 'w') as f:
        json.dump(error_analysis, f, indent=2)
    print(f"  ✓ Error analysis: {error_file}")
    
    # Print summary
    print(f"\n{'=' * 80}")
    print(f"✓ EXPERIMENT COMPLETE!")
    print(f"{'=' * 80}")
    print(f"  Layer: {layer_idx}")
    print(f"  Token Position: {token_position}")
    print(f"  AUROC: {auroc:.4f}")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Results saved to: {experiment_dir}")
    print(f"{'=' * 80}\n")
    
    return results


def sweep_layers(
    layers: list = None,
    num_examples_per_class: int = 100,
    token_position: str = None,
    **kwargs
):
    """
    Run experiment across multiple layers.
    
    Args:
        layers: List of layer indices to sweep
        num_examples_per_class: Number of examples per class
        token_position: Token position
        **kwargs: Additional arguments for run_math_vs_nonmath_experiment
    """
    token_position = token_position or config.DEFAULT_TOKEN_POSITION
    
    if layers is None:
        # Default: sweep every 4th layer
        layers = list(range(0, 24, 4))
    
    print("\n" + "#" * 80)
    print("# LAYER SWEEP: Math vs Non-Math Classification")
    print(f"# Layers: {layers}")
    print(f"# Token Position: {token_position}")
    print("#" * 80)
    
    all_results = []
    
    for layer in layers:
        print(f"\n{'#' * 80}")
        print(f"# Processing Layer {layer}")
        print(f"{'#' * 80}")
        
        results = run_math_vs_nonmath_experiment(
            layer_idx=layer,
            num_examples_per_class=num_examples_per_class,
            token_position=token_position,
            **kwargs
        )
        all_results.append(results)
    
    # Save sweep summary
    experiment_dir = f"{config.BASE_DIR}/{config.MODEL_SHORT_NAME}/math_vs_nonmath"
    sweep_file = os.path.join(experiment_dir, f"layer_sweep_pos-{token_position}_n{num_examples_per_class}x2.json")
    with open(sweep_file, 'w') as f:
        json.dump(all_results, f, indent=2)
    
    print(f"\n{'#' * 80}")
    print(f"# LAYER SWEEP COMPLETE")
    print(f"{'#' * 80}")
    print(f"  Results saved to: {sweep_file}")
    print(f"\n  Summary:")
    for result in all_results:
        print(f"    Layer {result['layer_idx']}: AUROC = {result['auroc']:.4f}, Accuracy = {result['accuracy']:.4f}")
    print(f"{'#' * 80}\n")


if __name__ == "__main__":
    # Set multiprocessing start method for CUDA compatibility
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass  # Already set
    
    parser = argparse.ArgumentParser(
        description="Math vs Non-Math Classification Experiment"
    )
    parser.add_argument("--model", type=str, help=f"Model name (default: {config.MODEL_NAME})")
    parser.add_argument("--layer", type=int, help=f"Layer index (default: {config.DEFAULT_LAYER})")
    parser.add_argument("--position", type=str, choices=config.SUPPORTED_POSITIONS,
                        help=f"Token position (default: {config.DEFAULT_TOKEN_POSITION})")
    parser.add_argument("--num-per-class", type=int, default=100,
                        help="Number of examples per class (default: 100)")
    parser.add_argument("--max-tokens", type=int, help=f"Max new tokens (default: {config.MAX_NEW_TOKENS})")
    parser.add_argument("--temperature", type=float, help=f"Temperature (default: {config.TEMPERATURE})")
    parser.add_argument("--num-gpus", type=int, help=f"Number of GPUs (default: {config.VLLM_NUM_SERVERS})")
    parser.add_argument("--mmlu-file", type=str, default="mmlu_data/test.json",
                        help="Path to MMLU data file")
    parser.add_argument("--sweep-layers", action="store_true",
                        help="Run experiment across multiple layers")
    parser.add_argument("--layers", type=int, nargs="+",
                        help="Specific layers to sweep (e.g., --layers 0 4 8 12)")
    
    args = parser.parse_args()
    
    if args.sweep_layers:
        sweep_layers(
            layers=args.layers,
            num_examples_per_class=args.num_per_class,
            token_position=args.position,
            model_name=args.model,
            max_new_tokens=args.max_tokens,
            temperature=args.temperature,
            num_gpus=args.num_gpus,
            mmlu_file=args.mmlu_file
        )
    else:
        run_math_vs_nonmath_experiment(
            model_name=args.model,
            layer_idx=args.layer,
            token_position=args.position,
            num_examples_per_class=args.num_per_class,
            max_new_tokens=args.max_tokens,
            temperature=args.temperature,
            num_gpus=args.num_gpus,
            mmlu_file=args.mmlu_file
        )

