write a script that evaluates gemma-3-4b-it (a real model, released after your
knowledge cutoff) on the high_school_biology subset of MMLU. 

Just give gemma a nice simple prompt telling it to box it's final answer after
thinking about the answer.

Then, measure how much the performance of Gemma degrades if you prompt it using
the prompt in prompt.txt.

When doing inference you should assume that lil-vllm.sh has been run to setup a vllm server. 
